# ğŸ§  Generative AI Notes for 2025

## ğŸ“˜ Table of Contents

1. [What is Generative AI?](#1-what-is-generative-ai)
2. [Why Use Generative AI?](#2-why-use-generative-ai)
3. [How Does Generative AI Work?](#3-how-does-generative-ai-work)
4. [Architecture of Generative AI Models (Transformers)](#4-architecture-of-generative-ai-models-transformers)
5. [Introduction to LLMs](#5-introduction-to-llms)
6. [How does LLM Work?](#6-how-does-llms-work)
7. [What Was the Process of Text Generation Before LLMs?](#7-what-was-the-process-of-text-generation-before-llms)
8. [LLM Models and Their Capabilities](#8-llm-models-and-their-capabilities)
9. [GPT Models VS Reasoning Models](#9-gpt-models-vs-reasoning-models)
10. [Token, Context, Context Window & Inference](#10-token-context-context-window--inference)
11. [Introduction of Prompt Engineering](#11-introduction-of-prompt-engineering)
12. [Types of Prompting in LLMs](#12-types-of-prompting-in-llms)
13. [Invoking the LLM](#13-invoking-the-llm)
14. [About System Prompt](#14-about-system-prompt)
15. [LLM Settings / Parameters Explained](#15-llm-settings--parameters-explained)
16. [Structured Output in LLMs](#16-structured-output-in-llms)
17. [Introducing Tool Calling](#17-introducing-tool-calling)
18. [Adding Memory to Chatbot](#-18-adding-memory-to-chatbot)
19. [Overview of RAG](#19-overview-of-rag)
20. [Overview of Agentic AI]()

---

## 1. What is Generative AI?

**Generative AI** is a type of **Artificial Intelligence (AI)** that can **create new content**, such as **text, images, audio, video, or even code**.

It works by combining the power of **Machine Learning (ML)** and **Deep Learning (DL)**.  
These models learn from large datasets and can then **generate original outputs** that mimic or extend the patterns in the training data.

---

## 2. Why Use Generative AI?

The main purpose of **Generative AI** is to **enhance productivity and creativity** by automating tasks that usually require human input.

### ğŸ’¡ Real-Life Use Cases

1. **Faster Content Creation:**  
   Generate blogs, social media posts, and email templates within seconds.

2. **Code Generation & Programming Assistance:**  
   Helps in writing code, fixing bugs, and generating functions automatically.

3. **Design & Creativity:**  
   Create images, logos, music, or even 3D models with minimal manual effort.

4. **Automation:**  
   Power chatbots, AI assistants, and automated report generation to save time and resources.

---

## 3. How Does Generative AI Work?

### ğŸ§  The Brain of AI â€” Neural Networks

A **Neural Network** is a system inspired by how the **human brain** works.  
It processes information through interconnected nodes called **neurons**.

<img src="./assets/Neural-Networks-Architecture.png" width="400px">

### ğŸ§© Structure of a Neural Network

- **Input Layer:** Receives raw data
- **Hidden Layers:** Learn patterns and relationships
- **Output Layer:** Produces the final prediction or result

**Example:**

```
If we ask the AI to identify photos of cats and dogs,
the neural network learns features such as ear shape, whiskers, and color patterns â€”
then predicts: â€œThis is a Cat.â€
```

### âš™ï¸ From Neural Networks â†’ Deep Learning â†’ Transformers

Traditional neural networks struggled with **sequential data** (like text),  
because the **meaning of words changes based on context**.

**Example:**

```
The word â€œbankâ€ could mean a river bank or a money bank â€”
so the model needs to understand the context.
```

Thatâ€™s where the hero comes in â€” the **Transformer Architecture**  
(introduced by Google in 2017 through the paper _â€œAttention is All You Needâ€_).  
This architecture handles **context understanding** using the **Self-Attention Mechanism**.

### âœ¨ Self-Attention Mechanism â€” The Real Magic

**Self-Attention** means the model looks at every word in relation to all other words in the sentence.  
It figures out how important each word is to understanding the others.

**Example:**

```
Sentence: â€œThe cat sat on the mat.â€


The model checks relationships like:
- â€œcatâ€ â†’ related to â€œsatâ€, â€œmatâ€
- â€œmatâ€ â†’ used in the context of â€œonâ€

By analyzing these relationships, the model builds **contextual understanding**,
allowing it to generate more meaningful and accurate results.
```

---

## 4. Architecture of Generative AI Models (Transformers)

The **Transformer architecture** is the backbone of almost all modern Generative AI models â€”  
like **GPT (OpenAI)**, **Gemini (Google)**, **Claude (Anthropic)**, and **LLaMA (Meta)**.

Introduced in 2017 by Google in the paper _â€œAttention is All You Needâ€_,  
Transformers revolutionized how machines understand and generate language.

<img src="./assets/transformer-architecture.png" width="450px">

### ğŸ§© Core Components of a Transformer

A typical Transformer model has two main parts:

#### 1. **Encoder**

- Takes input data (like a sentence) and converts it into **contextual embeddings**.
- Understands the **meaning and relationships** between words.
- Used in models like **BERT** (for understanding language).

#### 2. **Decoder**

- Takes the encoderâ€™s output (or its own previous output) to **generate new text or data**.
- Used in models like **GPT** (for generating text).

> ğŸ§  Some models use both (Encoderâ€“Decoder),  
> while others use only one part depending on their purpose.

### âš™ï¸ Inside Each Encoder/Decoder Block

Each block contains:

1. **Self-Attention Layer:**  
   Learns which words or tokens are important in relation to others.  
   Example â€” In the sentence _â€œThe cat sat on the matâ€_,  
   the model knows â€œcatâ€ relates more to â€œsatâ€ than to â€œmatâ€.

2. **Feed Forward Neural Network (FFN):**  
   Processes the attention output and refines the learned representation.

3. **Layer Normalization:**  
   Stabilizes and speeds up training by keeping values balanced.

4. **Residual Connections:**  
   Help prevent information loss by adding input values back into outputs (skip connections).

### ğŸ”„ Encoderâ€“Decoder Workflow

Input Text â†’ Encoder â†’ Context Representation â†’ Decoder â†’ Generated Output

```
Example:

Input: "Translate 'Hello' to Spanish"
Encoder understands â†’ meaning of "Hello"
Decoder generates â†’ "Hola"
```

---

## 5. Introduction to LLMs

**LLM** stands for **Large Language Model**. It is a type of **Generative AI model** specifically trained to **understand and generate human language (text)**.

### ğŸ§  Important Concepts

- **Scale Matters:** The word _â€œLargeâ€_ refers to models containing **billions or even trillions of parameters**. These **parameters** represent the modelâ€™s internal â€œknowledgeâ€ â€” how it maps words, meanings, and context together.
- **Trained on Massive Text Data:** LLMs are trained on enormous datasets including **books, websites, articles, code, and conversations**, allowing them to develop a strong understanding of **real-world knowledge and language structure**.
- **Beyond Text Generation:** LLMs are capable of performing a wide range of tasks such as **summarization, translation, question answering, code generation, and reasoning**.

### âš™ï¸ Example

- **GPT-3:** ~175 Billion parameters
- **GPT-4:** Estimated to have **trillions of parameters** (exact numbers are undisclosed)

ğŸ§© **In short:** Large Language Models are the core engines behind todayâ€™s AI systems, combining massive scale, deep learning, and contextual understanding to perform complex language-based tasks with human-like intelligence.

---

## 6. How does LLMs Work?

An **LLM (Large Language Model)** is essentially a **next-word prediction machine**, but at a highly intelligent level.

### âš™ï¸ Flow of Operation

1. You provide a **prompt** â€” for example: _â€œDelhi is the capital of â€¦â€_
2. The model uses its **training data** and **parameters** to predict the most likely next word â€” in this case, _â€œIndiaâ€_.
3. This prediction process continues **word by word**, using previous outputs as context, until the **complete response** is generated.

ğŸ§© **In short:** LLMs generate text by predicting the next word based on probability and context â€” repeating this process millions of times to form coherent, human-like language.

---

## 7. What Was the Process of Text Generation Before LLMs?

Before **LLMs**, text generation relied on **rule-based** and **statistical models**. These systems used predefined language rules and probability patterns but had **limited context understanding**.

### ğŸ§© Main Techniques Used Before LLMs

1. **Rule-Based Models (Pre-2000s)**

   - Manually defined â€œif-thenâ€ rules.
   - Example: If input = â€œHelloâ€, response = â€œHi! How are you?â€
   - âŒ No creativity â€” only fixed responses.

2. **Statistical Language Models (2000s)**

   - Used **n-grams** (word sequences) to predict the next word.
   - Example: â€œI love â€¦â€ â†’ predicts â€œIndiaâ€ if it appears frequently in data.
   - âŒ Could remember only short context (2â€“3 words).

3. **RNNs (Recurrent Neural Networks â€“ 2010s)**

   - Could remember previous words for short sequences.
   - Example: â€œI am going to New â€¦â€ â†’ predicts â€œYorkâ€.
   - âŒ Struggled with long sentences and slow training.

4. **LSTMs & GRUs (2014â€“2017)**

   - Improved memory and stability over RNNs.
   - Used in early chatbots like Siri and Google Assistant.
   - âŒ Still failed at long context and creative text generation.

5. **Transformers (2017 â€“ â€œAttention is All You Needâ€)**
   - Introduced **Self-Attention Mechanism**, enabling long-context understanding.
   - Formed the base for modern **LLMs** like **GPT**, **BERT**, and **T5**, starting the true AI revolution.

---

## 8. LLM Models and Their Capabilities

Here are some of the most popular **Large Language Models (LLMs)** and what they are best known for ğŸ‘‡

---

### ğŸ¤– GPT (Generative Pre-trained Transformer) â€“ _OpenAI_

- **GPT-2 (2019):** First widely known model to generate coherent text; still made random errors.
- **GPT-3 (2020):** 175B parameters; capable of essays, blogs, coding, and summarization.
- **GPT-4 (2023):** Multimodal (text + image), strong in reasoning, coding, and problem-solving.

### ğŸ§  BERT (Bidirectional Encoder Representations from Transformers) â€“ _Google, 2018_

- Focused on **understanding** text, not generating.
- Great for sentiment analysis, Q&A, and text classification.
- ğŸ† Known as the â€œunderstandingâ€ model of NLP.

### ğŸ”¤ T5 (Text-to-Text Transfer Transformer) â€“ _Google, 2020_

- Converts every NLP task into a **text-to-text** format.
- Example: _â€œTranslate English to Hindi: How are you?â€ â†’ â€œAap kaise ho?â€_
- Used for translation, summarization, and Q&A.

### ğŸŒ BLOOM â€“ _BigScience, 2022 (Open Source)_

- **Multilingual model** (supports 46 languages).
- Open-source alternative to GPT, handles mixed-language prompts easily.

### ğŸ¦™ LLaMA â€“ _Meta, 2023_

- Lightweight and **developer-friendly** models.
- Runs efficiently on personal machines.
- Open-source versions like **LLaMA-2** are great for AI app development.

### ğŸª¶ Claude â€“ _Anthropic, 2023_

- Designed for **safety** and **long-context** understanding.
- Can process 100k+ tokens â€” perfect for summarizing long documents or research papers.

### âš¡ Falcon â€“ _TII, 2023 (Open Source)_

- **Fast and cost-efficient** LLM.
- Competitive with GPT-3.5 in text generation and ideal for fine-tuning business applications.

---

## 9. GPT Models vs Reasoning Models

### GPT Models (Generative Models)

**Definition:** GPT models are primarily built to **generate text** by predicting the next word based on a prompt.

**Capabilities:** Text generation (blogs, stories, code), summarization, translation, chatbot responses, basic logical Q&A.

**Limitations:** Shallow reasoning â€” can be confidently wrong (hallucinations); mostly limited to patterns from training data.

### Reasoning Models (Next-Gen AI)

**Definition:** Reasoning models are designed for **logical thinking and step-by-step problem solving**; they follow chains of thought rather than only predicting likely next words.

**Examples:** OpenAIâ€™s reasoning series, Anthropicâ€™s advanced Claude variants, DeepMindâ€™s Gemini (reasoning-focused versions).

**Capabilities:** Multi-step math and logical problem solving, complex puzzle handling, code debugging with reasoning, scientific planning and decision support.

**In short:** GPT models excel at fluent generation; reasoning models excel at structured, multi-step thinking and more reliable reasoning.

---

## 10. Token, Context, Context Window & Inference

These four terms form the foundation of how LLMs work.

### ğŸ§© Token

A **token** is the smallest piece of text an AI model can understand.  
Example:

- "I love India" â†’ [ "I", " love", " India" ] â†’ 3 tokens
- "Iâ€™m learning AI" â†’ [ "I", "â€™m", " learning", " AI" ] â†’ 4 tokens

ğŸ‘‰ Models donâ€™t process text directly â€” they convert it into tokens (numbers/vectors) that machines can understand.

### ğŸ§  Context

**Context** = the information that the model uses to understand the meaning.
Example:

- User: Who is Virat Kohli?
- AI: Virat Kohli is an Indian cricketer.
- User: What is his wifeâ€™s name?

Here, â€œhisâ€ refers to _Virat Kohli_ â€” that link comes from context.

### ğŸ“ Context Window

The **context window** defines how many tokens a model can remember at once.  
Itâ€™s like the modelâ€™s **short-term memory**.

| Model            | Context Window Size |
| ---------------- | ------------------- |
| GPT-3            | 4K tokens           |
| GPT-4            | 8Kâ€“32K tokens       |
| GPT-4 Turbo / 4o | up to 128K tokens   |
| Claude 3 Opus    | up to 200K tokens   |
| Gemini 1.5 Pro   | up to 1M tokens ğŸ˜²  |

### âš™ï¸ Inference

**Inference** = when a trained model generates output based on your input.
Example:

- Prompt â†’ â€œWrite a poem about Delhi rain.â€
- Model â†’ uses trained weights + context + tokens
- Output â†’ â€œThe rain in Delhi whispers softly over streets of goldâ€¦â€

No training happens here â€” only **prediction** based on learned knowledge.

---

## 11. Introduction of Prompt Engineering

**Prompt engineering** is the process of crafting instructions for AI models (LLMs) so they produce the desired output. Clear, structured, and context-rich prompts lead to more accurate results.

- **Prompt** = input given to the AI model
- **Prompt engineering** = the art and science of designing effective prompts

### Why Prompt Engineering Matters

Prompt engineering is essential to unlock the full potential of LLMs.

**Key Benefits:**

- Control model behavior (tone, role, style, structure)
- Produce higher-quality and more accurate outputs
- Reduce hallucinations (incorrect or fabricated responses)
- Save tokens, time, and cost by improving efficiency
- Reuse and adapt good prompts across multiple tasks

Example:

- **Prompt 1:** â€œWrite about Virat Kohli.â€
- **Prompt 2:** â€œAct as a sports journalist and write a 100-word analytical summary of Virat Kohliâ€™s captaincy style.â€

The second prompt is more specific and role-based, so it yields a more focused and controlled output. This is the difference between a plain prompt and an engineered prompt.

---

## 12. Types of Prompting in LLMs

There are three foundational prompting styles: **Zero-Shot**, **Few-Shot**, and **Chain-of-Thought (CoT)**.

### Zero-Shot Prompting

You give the model a task with **no examples**; it relies solely on its training.

**Pros:** Simple, fast, good for general tasks.  
**Cons:** Can be vague or incorrect on complex tasks.

**Example:**

> Prompt: â€œTranslate to Hindi: â€˜AI will change the world.â€™â€  
> Output: â€œAI duniya ko badal dega.â€

### Few-Shot Prompting

You provide **a few examples (2â€“5)** in the prompt so the model learns the desired pattern.

**Pros:** Better for structured/custom tasks; reduces hallucination.  
**Cons:** Longer prompts and higher token cost.

**Example:**

```
Prompt:

Dog â†’ Kutta
Cat â†’ Billi
Book â†’ Kitaab
Pen â†’ ?

Output: â€œKalamâ€
```

### Chain-of-Thought (CoT) Prompting

You ask the model to **reason step-by-step** before giving the final answer. Best for multi-step reasoning.

**Pros:** Improves accuracy on math, logic, and complex reasoning; gives explainability.  
**Cons:** Longer outputs and slower responses.

**Example:**

```
Prompt: â€œIf 3 apples cost â‚¹60, how much do 10 apples cost? Think step-by-step.â€

Output:
Step 1: 3 apples = â‚¹60 â†’ 1 apple = â‚¹20
Step 2: 10 apples = 10 Ã— â‚¹20 = â‚¹200
Final Answer: â‚¹200
```

---

## 13. Invoking the LLM

This section explains how to invoke LLMs in your code using **Groq Cloud**, a cloud-based platform providing high-performance AI accelerators and LLMs, so you can run models efficiently without local hardware.

**Analogy:**  
Groq Cloud acts as an AI â€œsupercomputerâ€ on demand:

- No need to buy a powerful local GPU
- Simply call the API over the internet â†’ get instant AI output

### Basic Chat Completion API Example:

```javascript
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
  const completion = await groq.chat.completions.create({
    model: "llama-3.3-70b-versatile", // LLM model to invoke
    messages: [
      {
        role: "user", // roles: "user", "system", "assistant"
        content: "Hi", // Input text
      },
    ],
  });

  console.log(completion.choices[0].message);
}

main();
```

### Understanding the Response Object

```javascript
{
  "id": "chatcmpl-xxxx",
  "object": "chat.completion",
  "created": 1759940223,
  "model": "llama-3.3-70b-versatile",
  "choices": [
    {
      "index": 0,
      "message": { ... },   // Generated message from LLM
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": { ... },
  "system_fingerprint": "fp_xxxx",
  "x_groq": { "id": "req_xxx" },
  "service_tier": "on_demand"
}
```

**Key Fields:**

- choices: Array of generated outputs
- finish_reason: "stop" â†’ model finished naturally
- usage:
  - queue_time: time spent waiting
  - prompt_tokens: tokens in input
  - completion_tokens: tokens in output
  - total_tokens: input + output tokens
  - total_time: total request duration

> Always refer to the official documentation, as API responses and invocation methods may change over time.

---

## 14. About System Prompt

A **System Prompt** is a special message given to an LLM at the start of a conversation to set its **role, behavior, and style**.

```javascript
messages: [
  {
    role: "system",
    content: "You are Askly, a smart personal assistant.",
  },
  {
    role: "user",
    content: "Who are you?",
  },
];
```

### Effects of System Prompt

1. **Set Persona:**

   - Example: "Askly, smart personal assistant" â†’ model responds in a friendly, informative, and helpful tone.
   - Changing the system prompt (e.g., "You are a harsh critic") alters tone and style.

2. **Guide Conversation Context:**

   - The model answers future user questions according to the system prompt's context.

3. **Instruction-Based Control:**

   - Provides task instructions or behavior rules to the model.

---

## 15. LLM Settings / Parameters Explained

```javascript
async function main() {
  const completion = await groq.chat.completions.create({
    temperature: 1,
    top_p: 0.2,
    stop: "ga",
    max_completion_tokens: "",
    model: "llama-3.3-70b-versatile",
    frequency_penalty: 1,
    presence_penalty: 1,
    messages: [...],
  });
}
```

### 1. Temperature

Controls model randomness (0 to 2). Higher â†’ more creative but inconsistent.

| Temperature | Effect                     |
| ----------- | -------------------------- |
| 0           | Deterministic, always same |
| 0.5         | Slight randomness          |
| 1           | Standard, creative         |
| >1          | Highly random              |

**Use Cases:**

- `0` â†’ exact answers, classification tasks
- `0.7-1` â†’ creative writing, marketing content
- `> 1` â†’ brainstorming, jokes, unpredictable output

### 2. top_p (Nucleus Sampling)

Chooses words from top cumulative probability p.

**Example:**

```
â€œI want to eat â€¦â€
```

Next probabilities of model something like that ğŸ‘‡

| Word      | Probability |
| --------- | ----------- |
| pizza     | 0.50        |
| burger    | 0.30        |
| pasta     | 0.10        |
| ice-cream | 0.05        |
| rock      | 0.02        |
| car       | 0.01        |
| banana    | 0.01        |
| others    | 0.01        |

**Use Case:** Controlled randomness for content generation, summarization, chatbots.

### 3. stop

Defines sequences where generation stops.

**Use Cases:**

- Chatbot â†’ "\nUser:"
- Sentiment classification â†’ single-word output
- Code generation â†’ stop at `;` or `}`

### 4. max_completion_tokens

Maximum tokens model can generate.

**Use Cases:**

- Short response â†’ 20 tokens
- Long story â†’ 500+ tokens

### 5. frequency_penalty

Reduces repetition of words/phrases.

- Higher â†’ less repetition
- Lower â†’ repetition allowed

**Use Cases:** Creative writing, blogs, stories.

### 6. presence_penalty

Penalizes words already appeared, encourages new topics.

- Higher â†’ more new words, less repetition
- Lower â†’ model may repeat familiar words

---

## 16. Structured Output in LLMs

When we want an LLM to produce output in a specific format or structure (like JSON, key-value pairs, tables, or lists), itâ€™s called **Structured Output**.

Example:

```
Prompt:
  â€œSummarize the below review.â€
  â€œThis phone is great! The camera is amazing and the battery lasts all day.â€

Model Output (normal):
  â€œThe phone has an excellent camera and long battery life. Overall, a great product.â€
```

This output is human-readable but **not machine-readable**.  
If you need to extract data like sentiment, rating, or pros/cons in the backend, it becomes difficult.

Example (Structured Output):

```
Prompt:
â€œSummarize the review and return JSON with keys: sentiment, pros, cons.â€

Model Output (structured):
{
  "sentiment": "positive",
  "pros": ["amazing camera", "long battery life"],
  "cons": []
}
```

This output is **machine-readable**, easy to parse, and can be stored in the backend, APIs, or databases.

---

### How to Get Structured Output

#### 1. Enforce Structure Using a System Prompt

You can explicitly instruct the model: â€œAlways respond in JSON format.â€

Example:

```
{
role: "system",
content: `You are a helpful assistant that returns structured data only in JSON format.
When given a product review, return a JSON object with fields:

sentiment: positive, neutral, or negative

pros: list of positives

cons: list of negatives`
}
```

#### 2. JSON Mode (Structured Output via API Parameter)

Platforms like **Groq**, **OpenAI**, and **Anthropic** provide a special parameter:

> response_format: { type: "json_object" }

This enforces the model to strictly return pure JSON output.

---

### Project: An Interview Grade LLM:

```javascript
import Groq from "groq-sdk/index.mjs";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
  const completion = await groq.chat.completions.create({
    model: "llama-3.3-70b-versatile",
    messages: [
      {
        role: "system",
        content: `You are an interview grader assistant. Your tast is to generate candidate evaluation score.
        Output must be following JSON structure:
        {
          "confidence": number (1-10 scale),
          "accuracy": number (1-10 scale),
          "pass": boolean (true or false),
        }

        The response must:
          1. Include ALL fields shown above
          2. Use only the exact field names shown
          3. Follow the exact data types specified
          4. Contain ONLY the JSON object and nothing else 
        `,
      },
      {
        role: "user",
        content: `Q: What does === do in JavaScript?
          A: It checks strict equality-both value and type must match.

          Q: How do you create a promise that resolves after 1 second?
          A: const p = new Promise(r => setTimeout(r, 1000));

          Q: What is hoisting?
          A: JavaScript moves declarations (but not initialization) to top of their scope before code runs.

          Q: Why use let instead of var?
          A: let is block-scoped, avoiding the function-scope quirks and re-declaration issues of var.
        `,
      },
    ],
  });

  console.log(completion.choices[0].message.content);
}

main();
```

---

## 17. Introducing Tool Calling

**Tool Calling** is a powerful feature in LLMs that allows them to interact with **external resources** such as APIs, databases, or the web.

<img src="./assets/tool-call-flow.png" width="450px">

### Why Tool Calling is Important

| Use Case                     | Description                                                                      |
| ---------------------------- | -------------------------------------------------------------------------------- |
| ğŸŒ¦ï¸ **Real-time Data**        | To fetch live information such as weather, stock prices, or news                 |
| ğŸ—“ï¸ **System Integration**    | To access system-level tools like calendar, email, or file systems               |
| ğŸ§® **Custom Logic**          | To perform mathematical operations or database queries                           |
| ğŸ§° **Multi-step Automation** | To trigger multiple tools and automate workflows (similar to Zapier automations) |

Tool Calling bridges the gap between **language understanding** and **action execution**, enabling LLMs to not just talk â€” but actually _do_ things in the real world.

### Tool Calling: Setup

```javascript
import Groq from "groq-sdk/index.mjs";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

async function main() {
  const completion = await groq.chat.completions.create({
    model: "llama-3.3-70b-versatile",
    temperature: 0,
    messages: [
      {
        role: "system",
        content: `You are a smart personal assistant who answers the asked questions.
                  You have access to following tools:
                  1. webSearch({query}: {query: string}) //Search the latest information and realtime data on the internet.`,
      },
      {
        role: "user",
        content: `Who is the Current Indian Test Cricket Team Captain?`,
      },
    ],
    tools: [
      {
        type: "function",
        function: {
          name: "webSearch",
          description:
            "Search the latest information and realtime data on the internet.",
          parameters: {
            type: "object",
            properties: {
              query: {
                type: "string",
                description: "The search query to perform search on.",
              },
            },
            required: ["query"],
          },
        },
      },
    ],
    tool_choice: "auto",
  });

  let toolCalls = completion.choices[0].message.tool_calls;

  if (!toolCalls) {
    console.log(`Assistant: ${completion.choices[0].message.content}`);
    return;
  }

  for (const tool of toolCalls) {
    const functionName = tool.function.name;
    const functionArgs = tool.function.arguments;

    if (functionName === "webSearch") {
      const toolResult = await webSearch(JSON.parse(functionArgs));
      console.log("Tool Result: ", toolResult);
    }
  }
}

main();

async function webSearch({ query }) {
  console.log("Calling a web search...");
  // Here we call api for getting real time updates
  return `The current Captain of Indian Text Cricket Team is Shubman Gill.`;
}
```

### Tool Calling: Implement Tool

```javascript
import Groq from "groq-sdk/index.mjs";
import { tavily } from "@tavily/core";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });
const tvly = tavily({ apiKey: process.env.TAVILY_API_KEY });

async function main() {
  const messages = [
    {
      role: "system",
      content: `You are a smart personal assistant who answers the asked questions.
                  You have access to following tools:
                  1. webSearch({query}: {query: string}) //Search the latest information and realtime data on the internet.`,
    },
    {
      role: "user",
      content: `When was iphone 17 launched in india?`,
    },
  ];

  while (true) {
    const completion = await groq.chat.completions.create({
      model: "llama-3.3-70b-versatile",
      temperature: 0,
      messages: messages,
      tools: [
        {
          type: "function",
          function: {
            name: "webSearch",
            description:
              "Search the latest information and realtime data on the internet.",
            parameters: {
              type: "object",
              properties: {
                query: {
                  type: "string",
                  description: "The search query to perform search on.",
                },
              },
              required: ["query"],
            },
          },
        },
      ],
      tool_choice: "auto",
    });

    // WHEN RESULT COMES OF COMPLETION ONE PUSH INTO MESSAGES
    messages.push(completion.choices[0].message);

    let toolCalls = completion.choices[0].message.tool_calls;

    if (!toolCalls) {
      console.log(`Assistant: ${completion.choices[0].message.content}`);
      break;
    }

    for (const tool of toolCalls) {
      const functionName = tool.function.name;
      const functionArgs = tool.function.arguments;

      if (functionName === "webSearch") {
        const toolResult = await webSearch(JSON.parse(functionArgs));

        // PUSH THE TOOL CALL MESSAGE INTO MESSAGES ARRAY
        messages.push({
          tool_call_id: tool.id,
          role: "tool",
          name: functionName,
          content: toolResult,
        });
      }
    }
  }
}

main();

async function webSearch({ query }) {
  console.log("Calling a web search...");

  const response = await tvly.search(query);

  const finalResult = response.results
    .map((result) => result.content)
    .join("\n\n");

  return finalResult;
}
```

---

## ğŸ§  18. Adding Memory to Chatbot

In this section, we focus on adding memory to a Generative AI chatbot called Askly â€” enabling it to remember previous interactions within a conversation thread.

Without memory, each user query is treated as an independent request, and the LLM forgets previous context (for example: if you say â€œIâ€™m Mohitâ€ and later ask â€œWhatâ€™s my name?â€, it wonâ€™t remember).

<img src="./assets/chats.png" width="500px">

By implementing memory, the chatbot becomes context-aware, allowing it to recall user information, previous questions, and maintain a natural flow of conversation â€” just like ChatGPT.

### âš™ï¸ Why Memory is Important

| Without Memory                     | With Memory                                           |
| ---------------------------------- | ----------------------------------------------------- |
| Each message is independent        | Maintains context between messages                    |
| Chatbot forgets past user input    | Chatbot remembers userâ€™s name, interests, and context |
| Unnatural and repetitive responses | Smooth, conversational experience                     |
| No personalization                 | Personalized and engaging chat                        |

### ğŸ§© Approaches to Add Memory

There are two main ways to add memory to your chatbot:

1. In-Memory Cache (short-term, fast, simple)

   - Data is stored temporarily in RAM.
   - Lost when the server restarts.
   - Ideal for prototypes and development.

Example: node-cache.

2. Database Storage (persistent, scalable)

   - Data stored permanently in a database like MongoDB, Redis, or PostgreSQL.
   - Useful for production-level assistants that should remember users long-term.

### ğŸ§  Implementation Using Node-Cache

**1. Install Node-Cache**

```bash
npm install node-cache
```

**2. Import and Initialize Cache**

```javascript
import NodeCache from "node-cache";
const cache = new NodeCache({ stdTTL: 60 * 60 * 24 }); // Cache valid for 24 hours
```

stdTTL defines how long (in seconds) each cache entry remains valid (here, 24 hours).

**3. Asklyâ€™s LLM logic now includes memory retrieval and storage:**

```javascript
import NodeCache from "node-cache";
import Groq from "groq-sdk/index.mjs";
import { tavily } from "@tavily/core";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });
const tvly = tavily({ apiKey: process.env.TAVILY_API_KEY });

const cache = new NodeCache({ stdTTL: 60 * 60 * 24 }); // 24 hours

export async function generate(userMessage, threadId) {
  const baseMessages = [
    {
      role: "system",
      content: `You are Askly a smart personal assistant.
                  If you know answers to a question, answer it directly in plain English.
                  It the answer required real-time, local, or up-to-date information, or if you don't know the answer, use the available tools to find it....`,
    },
  ];

  // RETRIEVE CONVERSATION MEMORY FOR GIVE THREAD
  const messages = cache.get(threadId) ?? baseMessages;

  messages.push({
    role: "user",
    content: userMessage,
  });

  while (true) {
    const completion = await groq.chat.completions.create({
      model: "llama-3.3-70b-versatile",
      temperature: 0,
      messages: messages,
      tools: [
        {
          type: "function",
          function: {
            name: "webSearch",
            description:
              "Search the latest information and realtime data on the internet.",
            parameters: {
              type: "object",
              properties: {
                query: {
                  type: "string",
                  description: "The search query to perform search on.",
                },
              },
              required: ["query"],
            },
          },
        },
      ],
      tool_choice: "auto",
    });

    // WHEN RESULT COMES OF COMPLETION ONE PUSH INTO MESSAGES
    messages.push(completion.choices[0].message);

    let toolCalls = completion.choices[0].message.tool_calls;

    if (!toolCalls) {
      cache.set(threadId, messages); // STORE CONVERSATION IN MEMORY
      return completion.choices[0].message.content;
    }

    for (const tool of toolCalls) {
      const functionName = tool.function.name;
      const functionArgs = tool.function.arguments;

      if (functionName === "webSearch") {
        const toolResult = await webSearch(JSON.parse(functionArgs));

        // PUSH THE TOOL CALL MESSAGE INTO MESSAGES ARRAY
        messages.push({
          tool_call_id: tool.id,
          role: "tool",
          name: functionName,
          content: toolResult,
        });
      }
    }
  }
}

async function webSearch({ query }) {
  console.log("Calling a web search...");

  const response = await tvly.search(query);

  const finalResult = response.results
    .map((result) => result.content)
    .join("\n\n");

  return finalResult;
}
```

**Here is the Final output:**

<img src="./assets/chat-with-memory.png" width="500px">

---

## 19. Overview of RAG

A Technique where a model first retrieves relevant information from a knowledge souce, then use that information to generate an answer.

### How Things Work Without RAG

<img src="./assets/without-rag.png" width="500px">

### How Things Work With RAG

<img src="./assets/with-rag.png" width="500px">

### âš™ï¸ RAG Workflow (Step-by-Step)

1. User Query

   - The user asks a question.

     Example: â€œWhat is the 44 Moving Average strategy in trading?â€

2. Retriever

   - The retriever searches for relevant documents from an external knowledge base (PDFs, websites, databases, etc.)

   - This uses semantic search (based on meaning, not keywords).

3. Embedding

   - Both the user query and documents are converted into vectors (numerical representations) using an embedding model.

   - This allows the system to find semantically similar documents.

4. Context Preparation

   - The top relevant documents are added as context to the userâ€™s query.

5. LLM Generation

   - The LLM receives the query + retrieved context.

   - It then generates a fact-based, context-aware answer.

6. Final Answer

   - The user receives an accurate and explainable response.

---

## ğŸ§  Understanding Core Concepts Before Building a RAG System

Before building a Retrieval-Augmented Generation (RAG) system,
we first need to understand some core concepts â€” because everything in RAG revolves around data.

### 1. What is Data?

Everything starts from data â€” because AI learns, retrieves, and generates answers from it.

There are two main types of data ğŸ‘‡

| Type                  | Description                         | Example                                            |
| --------------------- | ----------------------------------- | -------------------------------------------------- |
| **Structured Data**   | Organized in fixed rows and columns | Employee records, student database, transactions   |
| **Unstructured Data** | Free-form, not stored in tables     | Company policies, PDFs, blog posts, emails, videos |

Structured data can be easily stored and searched in traditional databases like:

- MongoDB
- PostgreSQL
- MySQL

Because its format is well-defined (columns, rows, indexes),

you can run direct queries like:

```sql
SELECT * FROM employees WHERE department = 'Sales';
```

**The Challenge with Unstructured Data**

But unstructured data (like PDFs, articles, or policy docs) canâ€™t be stored or searched easily in tables.

Even if you store it, how will you search meaningfully inside it?

ğŸ‘‰ Example:
If you store an entire company policy as text, and you ask

> â€œHow many paid leaves do I get?â€

The system canâ€™t understand meaning â€” it only does keyword search,
so if â€œpaid leavesâ€ is written differently like â€œcompensated vacation,â€ it fails.

### 2. Enter: Similarity Search ğŸ”

Similarity Search means finding the most relevant or similar results
based on meaning, not just exact words.

ğŸ§  Example:

- You ask: â€œWho founded Apple?â€
- Document says: â€œApple was established by Steve Jobs and his team.â€
  Even though you didnâ€™t use the word â€œfounded,â€
  similarity search can still find it â€” because it matches the semantic meaning.

Itâ€™s used in:

- Google Lens (finding similar images)
- Spotify (finding similar songs)
- RAG (finding similar documents or knowledge)

### 3. Vector Database: The Backbone of Similarity Search

To perform similarity search, we need a vector database.

ğŸ§® Vector database stores vectors (numbers) that represent the meaning of text, images, etc.
Instead of matching words, it finds **nearest** meaning.

Examples of vector databases:

- ğŸ§  Pinecone
- âš™ï¸ ChromaDB
- ğŸ“¦ Weaviate
- ğŸ§© FAISS

### 4. What Are Vector Embeddings?

Now the main question â€” how do we get these â€œvectorsâ€?

ğŸ‘‰ Vector Embeddings are numerical representations of data that capture their semantic meaning.

```
Sentence: â€œApple is a tech company.â€
Vector: [0.21, 0.88, -0.43, ...]
```

```
Sentence: â€œMicrosoft builds software.â€
Vector: [0.22, 0.87, -0.41, ...]
```

Since both have similar meanings, their vectors are close in multi-dimensional space.

### 5. How Are Embeddings Created? (Machine Learning Models)

Embeddings are generated using machine learning models
trained to understand text meaning and context.

These models analyze huge datasets and learn which words or phrases mean similar things.

ğŸ§  Examples of embedding models:

- text-embedding-3-small (OpenAI)
- sentence-BERT
- Cohere-Embed
- MiniLM

These models convert text â†’ vector numbers.

### 6. Behind the Scenes: How Vector Embeddings Work

Each piece of text becomes a point in high-dimensional space (not just 2D or 3D).
For example, an embedding may have 768 or even 1536 dimensions.

- Similar meanings â†’ points close together
- Unrelated meanings â†’ points far apart

These clusters represent how words or documents are semantically related.

ğŸŒ€ Example visualization:

| Concept           | Nearby cluster              |
| ----------------- | --------------------------- |
| â€œdogâ€             | cat, puppy, animal          |
| â€œfinanceâ€         | money, banking, investment  |
| â€œapple (fruit)â€   | banana, mango, orange       |
| â€œapple (company)â€ | iPhone, MacBook, Steve Jobs |

Once embeddings are created, they are stored in a vector database along with:

- The original text/document
- Metadata (source, title, tags)

Then when a new query comes:

1. Query â†’ embedding
2. Search in vector DB â†’ finds top similar embeddings
3. Return relevant documents â†’ passed to LLM
4. LLM uses them to generate the final answer

### 7. Searching Efficiently â€” The Vector Index

Now imagine millions of embeddings â€” searching one by one would be slow.
So vector databases use a special data structure called a Vector Index
to quickly find nearest vectors.

Common vector indexing algorithms:

- HNSW (Hierarchical Navigable Small World)
- IVF (Inverted File Index)
- Annoy (Approximate Nearest Neighbor)

These help find â€œnearestâ€ vectors fast even in large datasets.

---

## 20. Overview of Agentic AI

### What is Agentic AI?

**Agentic AI** refers to AI systems that can **act autonomously** â€” meaning they can make decisions, take actions, and achieve goals **without constant human input**.

In simple terms:

> Traditional AI just _responds_ to your input,  
> but **Agentic AI can think, plan, and act on its own** to complete tasks.

**Example:**

Imagine you say:

> â€œBook me a flight to Mumbai next week and reserve a nearby hotel.â€

A **normal AI** (like ChatGPT) would just tell you _how_ to do it.  
But an **Agentic AI** would:

1. Search for available flights âœˆï¸
2. Compare prices ğŸ’°
3. Choose the best option âœ…
4. Book the ticket and confirm with you ğŸ’¬

Thatâ€™s an **AI Agent** â€” taking **actions**, not just generating answers.

### Key Abilities of Agentic AI

| Capability               | Description                                 | Example                                            |
| ------------------------ | ------------------------------------------- | -------------------------------------------------- |
| **Autonomy**             | Works without continuous human control      | Books flight and hotel automatically               |
| **Goal-Oriented**        | Understands your final goal and plans steps | Knows your goal is to â€œreach Mumbai comfortablyâ€   |
| **Reasoning & Planning** | Can think step-by-step                      | Decides which site gives the best deal             |
| **Tool Use**             | Can use APIs, browsers, or software tools   | Uses Skyscanner API to fetch flights               |
| **Memory**               | Remembers past actions and outcomes         | Learns your preferred airline or seat type         |
| **Collaboration**        | Can work with other AI agents               | One agent books flight, another arranges transport |

### Why Agentic AI is the Future

- Reduces human workload
- Automates multi-step complex workflows
- Can collaborate with humans and other AI systems
- A key building block for **Artificial General Intelligence (AGI)**

---
